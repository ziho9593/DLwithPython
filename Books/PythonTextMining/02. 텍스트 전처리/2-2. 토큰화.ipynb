{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2. 토큰화\n",
    "\n",
    "NLTK는 교욱용으로 개발된 자연어 처리 및 문서 분석용 파이썬 패키지로, WordNet을 비롯해 자연어 처리를 지원하는 다양한 라이브러리와 문서(말뭉치) 그리고 예제들을 제공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SystemsAnalysisLab\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     C:\\Users\\SystemsAnalysisLab\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package webtext is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SystemsAnalysisLab\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SystemsAnalysisLab\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\SystemsAnalysisLab\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 필요한 nltk 라이브러리 다운로드\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('webtext')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장 토큰화\n",
    "문장 토큰화는 여러 문장으로 이루어진 텍스트를 각 문장으로 나누는 것으로 nltk의 sent_tokenize를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello everyone.', \"It's good to see you.\", \"Let's start our text mining class!\"]\n"
     ]
    }
   ],
   "source": [
    "para = \"Hello everyone. It's good to see you. Let's start our text mining class!\"\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "print(sent_tokenize(para)) # 주어진 텍스트를 문장 단위로 토큰화. 주로 . ! ? 등을 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "내부적으로 sent_tokenize는 영어 학습 데이터에 대해 사전학습된 모델을 사용해 토큰화한다. 다른 언어에 대해 문장 토큰화를 하려면 아래와 같이 사전학습된 모델을 지정해 불러올 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Je t'ai demandé si tu m'aimais bien, Tu m'a répondu non.\", \"Je t'ai demandé si j'étais jolie, Tu m'a répondu non.\", \"Je t'ai demandé si j'étai dans ton coeur, Tu m'a répondu non.\"]\n"
     ]
    }
   ],
   "source": [
    "# 프랑스어를 학습한 모델을 사용하는 예\n",
    "\n",
    "paragraph_french = \"\"\"Je t'ai demandé si tu m'aimais bien, Tu m'a répondu non. \n",
    "Je t'ai demandé si j'étais jolie, Tu m'a répondu non. \n",
    "Je t'ai demandé si j'étai dans ton coeur, Tu m'a répondu non.\"\"\"\n",
    "\n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load(\"tokenizers/punkt/french.pickle\")\n",
    "print(tokenizer.tokenize(paragraph_french))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK에는 아직 한글에 대한 사전학습 모델이 없다. 그러나 문장 토큰화는 각 문장의 끝에 있는 마침표 등을 기준으로 분리하도록 학습되어 있으므로, 영어로 학습된 모델도 한국어에 대해 어느 정도 잘 작동할 것으로 예측 가능하다. 한글에 대한 작동 여부를 알아보기 위해 아래 예제를 실행해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요, 여러분.', '만나서 반갑습니다.', '이제 텍스트마이닝 클래스를 시작해봅시다!']\n"
     ]
    }
   ],
   "source": [
    "para_kor = \"안녕하세요, 여러분. 만나서 반갑습니다. 이제 텍스트마이닝 클래스를 시작해봅시다!\"\n",
    "\n",
    "print(sent_tokenize(para_kor)) #한국어에 대해서도 sentence tokenizer는 잘 동작함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 토큰화\n",
    "일반적으로 토큰화라고 하면 단어 토큰화(word tokenize)를 의미하며, 단어 토큰화는 대상이 되는 텍스트를 단어 단위로 분리하는 작업을 말한다. NLTK에서는 word_tokenize로 단어 토큰화를 할 수 있다. 단어 토큰화를 하려고 반드시 문장 토큰화를 수행할 필요는 없으며, 문장 단위의 분석이 필요할 때만 문장 토큰화를 하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'It', \"'s\", 'good', 'to', 'see', 'you', '.', 'Let', \"'s\", 'start', 'our', 'text', 'mining', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(word_tokenize(para)) # 주어진 text를 word 단위로 tokenize함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 결과에서 마침표나 느낌표가 별도의 단어로 분리되어 있으며, It's는 It과 's로 분리됨을 볼 수 있다. NLTK의 WordPunctTokenizer를 사용할 때 결과가 바뀌는지를 보기 위해 아래 코드를 실행해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'It', \"'\", 's', 'good', 'to', 'see', 'you', '.', 'Let', \"'\", 's', 'start', 'our', 'text', 'mining', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer  \n",
    "\n",
    "print(WordPunctTokenizer().tokenize(para))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_tokenize와 달리 It's를 It, ', s의 세 토큰으로 분리하는 것을 볼 수 있다. 이는 두 토크나이저(tokenizer)가 서로 다른 알고리즘에 기반하기 때문인데, 이에 사용자는 토크나이저의 특성을 파악하고 자신의 목적에 맞는 토크나이저를 선택할 필요가 있다.\n",
    "\n",
    "word_tokenizer를 한글에도 사용할 수 있는지 알아보기 위해 아래 코드를 실행해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요', ',', '여러분', '.', '만나서', '반갑습니다', '.', '이제', '텍스트마이닝', '클래스를', '시작해봅시다', '!']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(para_kor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영어를 기준으로 설명할 때에는 단어 단위로 분리한다고 설명할 수 있으나, 한글을 기준으로 할 때에는 명확하지 않게 느껴진다. 한글을 대상으로 하는 토큰화는 엄밀히 말하자면 의미를 가지는 최소단위인 형태소로 텍스트를 분리하는 것을 의미한다. 그런 관점에서 볼 때 위 결과는 어딘가 이상하게 느껴진다. '안녕하세요'를 '안녕'과 '하세요'로 나누는 것이 더 맞을 거 같다.\n",
    "\n",
    "영어는 보통 모든 단어를 공백으로 구분할 수 있어 어렵지 않게 단어 토큰화를 할 수 있지만, 한국어에서는 의미를 이루는 최소 단위가 공백 없이 붙어있는 경우가 많아 공백을 이용한 분리만으로는 부족하게 느껴진다.\n",
    "\n",
    "따라서 한국어 텍스트를 정확히 토큰화하려면 영어와는 다른 방법이 필요하다. 공백과 같이 단어를 구분해 주는 것들을 단어 경계(word boundary)라고 하는데, 공백만으로 토큰화가 잘 되지 않는다면 새로운 방법으로 단어를 분리해야 하며 이와 같은 작업을 단어 분할(word segmentation)이라고 한다. 한국어에 대해서는 다양한 단어 분할 방법을 적용할 필요가 있다. 파이썬 기반의 한글 토큰화 및 형태소 분석기는 KoNLPy가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정규표현식을 이용한 토큰화\n",
    "NTLK가 제공하는 함수를 이용해 토큰화를 하면 간편하다는 장점이 있으나, 내가 원하는 대로 세밀하게 토큰화하기 어렵다는 단점도 있다. 정규표현식을 이용하면 NLTK를 쓰지 않고도 다양한 조건에 따라 토큰화할 수 있다. 다만 정규표현식을 익혀야 한다는 부담은 있다.\n",
    "\n",
    "정규표현식은 regex 혹은 regexp라고 줄여서 표현되며, 문자열에 대해 원하는 검색 패턴을 지정하는 방법이다. 이 패턴은 보통 문자열을 검색하고 치환하는 데 사용된다. 예를 들어 'How are you?'에서 'are'만 검색하고 싶다면 검색어를 'are'로 주면 되는데, 나아가 'a'로 시작하는 모든 단어를 검색하고 싶을 때 쓸 수 있는 것이 정규표현식이다.\n",
    "\n",
    "정규표현식에서는 메타 문자로 패턴을 표현한다. 주로 사용되는 메타 문자 중 가장 먼저 알아둘 것은 문자 클래스인 []이다. 문자 클래스는 그 사이에 들어있는 문자와 매칭한다. [abc]라고 쓰면 a, b, c 중 하나라도 일치하는 문자들을 가져온다.\n",
    "\n",
    "파이썬에서 정규표현식을 지원하는 라이브러리는 re다. 정확한 이해를 위해 아래 예를 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "re.findall(\"[abc]\", \"How are you, boy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re 패키지의 findall() 함수는 첫째 인수의 패턴을 둘째 인수인 문자열에서 검색하여 매칭되는 모든 값들을 반환해준다. 따라서 위 결과 a, b는 각각 are과 boy의 첫 글자다.\n",
    "\n",
    "숫자를 찾고 싶다면 [0123456789]라고 써주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', '7', '5', '9']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[0123456789]\", \"3a7b5c9d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문자열에서 숫자들을 찾아 반환한 것을 볼 수 있다. [0123456789]를 줄여서 쓰고 싶다면 -를 사용해 [0-9]라고 쓰면 숫자가 된다. 마찬가지로 대소문자를 포함한 모든 알파벳을 검색하고 싶다면 [a-zA-Z]라고 쓰면 된다. 알파벳과 숫자, \\_까지 검색하고 싶다면 [a-zA-Z0-9_]라고 쓰면 되는데, 이에 대한 줄임 표현인 \\w가 있으니 길게 쓰지 않아도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', 'a', '7', 'b', '_', '5', 'c', '9', 'd']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[\\w]\", \"3a 7b_ '.^&5c9d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 예시에서 일부러 특수문자들을 섞었는데도 알파벳, 숫자, _를 잘 찾아냈다. \n",
    "\n",
    "다음으로 중요한 메타 문자는 +다. +는 한 번 이상의 반복을 의미한다. 예를 들어 문자열에서 _가 한 번 이상 반복된 부분을 찾고 싶다면 아래와 같이 쓸 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_', '__', '___']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[_]+\", \"a_b, c__d, e___f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\w에는 공백이나 쉼표 등이 포함되지 않는다. 이 특징을 이용하면 아래와 같이 주어진 문자열에서 공백이나 쉼표 등으로 구분되는 단어들을 찾아낼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How', 'are', 'you', 'boy']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[\\w]+\", \"How are you, boy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 결과를 보면, 지금까지 배운 단어 토큰화와 거의 같은 결과를 제공하는 것을 볼 수 있다. \n",
    "\n",
    "마지막으로 반복 횟수를 지정하는 방법이다. +가 1번 이상 반복을 나타내는 반면, {}를 이용하면 정확한 반복 횟수를 지정할 수 있다. o가 2~4회 반복된 문자열만 찾아내고 싶다면 아래와 같이 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oo', 'oooo', 'oooo', 'ooo']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[o]{2,4}\", \"oh, hoow are yoooou, boooooooy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 결과를 보면 oh의 o는 한 번이므로 검색되지 않았다. 반면 boooooooy는 o가 7개이므로 최대 개수인 앞 네 개가 매칭되고 남은 뒤의 세 개가 또 매칭됐다. 여기까지 이해했다면 이제 정규표현식을 이용한 토크나이저를 이해할 수 있게 됐다.\n",
    "\n",
    "NLTK에서는 정규표현식을 사용하는 토크나이저를 아래와 같이 RegexpTokenizer로 제공한다. RegexpTokenizer() 함수의 인수로 원하는 정규표현식을 주면 그에 따라 토큰화를 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sorry', 'I', \"can't\", 'go', 'there']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\") # regular expression(정규식)을 이용한 tokenizer, 문자니 숫자 혹은 '가 반복되는 것을 찾아냄\n",
    "\n",
    "print(tokenizer.tokenize(\"Sorry, I can't go there.\")) # 정규식에서 \\w과 '를 포함했으므로 can't를 하나의 단어로 인식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 예시에서 정규표현식으로 [\\w']+를 썼으므로 문자, 숫자, _ 외에 '까지 포함해 단어를 구분했기 때문에 can't를 하나의 단어로 인식했다. 즉, 그 외 문자는 단어를 구분하는 단어 경계로 사용된 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sorry', 'I', 'can', 't', 'go', 'there']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(\"[\\w]+\")\n",
    "\n",
    "print(tokenizer.tokenize(\"Sorry, I can't go there.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 결과에서 볼 수 있듯이 '를 정규표현식에서 빼면 can't는 can과 t로 나누어진다.\n",
    "\n",
    "좀 더 복잡하게 먼저 주어진 텍스트를 모두 소문자로 바꾸고 '를 포함하여 세 글자 이상의 단어들만 골라내고 싶다면, 메타 문자 {}에서 어떤 수 이상을 표현하는 방법을 이용해 뒤의 숫자를 아래와 같이 생략하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sorry', \"can't\", 'there']\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Sorry, I can't go there.\"\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w']{3,}\") \n",
    "\n",
    "print(tokenizer.tokenize(text1.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 노이즈와 불용어 제거\n",
    "\n",
    "정규표현식을 이용한 토큰화 과정을 보면, 그 과정에서 특수문자 등의 불필요한 문자들 혹은 노이즈가 삭제된 것을 볼 수 있다. 토큰화 과정과 별도로 정규표현식을 이용한 치환을 통해 원하는 패턴의 노이즈를 제거할 수도 있다.\n",
    "\n",
    "불용어는 의미 없는 특수문자 등과는 별개로, 실제 사용되는 단어이지만 분석에는 별 필요가 없는 단어들을 말한다. 영어의 경우에는 길이가 짧은 단어들을 삭제함으로 많은 불용어들을 삭제하는 것이 가능하다. 보통 불용어는 빈도가 너무 적거나 혹은 너무 많아서 별 필요가 없는 단어들이다. 그 외 분석의 목표 관점에서 볼 때 필요 없는 단어들도 존재한다.\n",
    "\n",
    "길이가 짧은 단어들은 위에서 살펴본 바와 같이 정규표현식을 사용해 쉽게 제거할 수 있다. 위 예와 같이 보통 길이가 3 미만인 단어들은 삭제하는 것이 일반적이다.\n",
    "\n",
    "그 외 특정 단어를 지정해 불용어 사전을 만들고, 사전을 참조해 불용어를 삭제할 수 있다. NLTK에서는 stopwords라는 라이브러리를 이용해 언어별 불용어 사전을 제공하며, 아래 예시는 NLTK의 영어 불용어 사전에 등재된 값을 사용해 불용어를 제거하는 것을 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sorry', 'go', 'movie', 'yesterday']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords # stopwords: 일반적으로 분석 대상이 아닌 단어들\n",
    "\n",
    "english_stops = set(stopwords.words('english')) # 반복이 되지 않도록 set으로 변환\n",
    "\n",
    "text1 = \"Sorry, I couldn't go to movie yesterday.\"\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "tokens = tokenizer.tokenize(text1.lower()) # word_tokenize로 토큰화\n",
    "\n",
    "result = [word for word in tokens if word not in english_stops] # stopwords를 제외한 단어들로만 list를 생성\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 결과를 보면 I, couldn't, to가 제거된 것을 볼 수 있다. 즉, 이 세 단어는 NLTK stopword에 포함돼 있다는 것을 의미한다. 불용어 사전을 출력해보면 어떤 단어들이 등록되어 있는지 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"isn't\", 'y', 'each', 'those', 'should', 'own', 'aren', 'itself', 'did', 'll', 'just', 'at', 'a', 'she', 'having', 'were', 'shan', 'very', 'mightn', 'if', 'before', 'after', 'don', 'doesn', 'you', 'who', 'about', 'further', 'few', 'under', 'have', 'how', \"should've\", 'why', 'through', \"weren't\", \"wouldn't\", 'yourself', 'was', \"shouldn't\", 'his', 'on', 'needn', 'couldn', 'than', 'then', 'both', 'when', 'am', \"you've\", 'so', 'mustn', 'our', 'by', \"it's\", \"won't\", 'being', 'to', 'what', 'ours', 'it', 'i', 'your', 'ma', 'm', 'will', 'has', 'with', 'any', 'be', 't', 'in', \"didn't\", 'between', 'because', \"doesn't\", 'hadn', \"she's\", 'once', 'he', \"don't\", 'd', 'this', 'here', 'not', \"needn't\", \"you're\", 'these', 'other', \"shan't\", \"haven't\", \"aren't\", 'where', \"couldn't\", 'some', 'too', 'haven', 'herself', 'they', 'ourselves', 'of', 'o', \"you'll\", 'had', 'ain', 'shouldn', 'an', 'during', \"hasn't\", 'which', 'does', 'up', 'or', 'him', 'do', 'yours', 'over', 'we', 've', 'been', 'no', 'off', 's', 'her', 'all', 'them', 'the', 'against', 'wasn', \"mightn't\", 'myself', \"you'd\", 'as', 'hers', 'my', 'same', 'hasn', 'more', 'isn', 'into', 'while', 'whom', \"wasn't\", 'nor', 'that', 'below', 'for', 'didn', 'from', 'down', 're', \"hadn't\", 'wouldn', 'is', 'their', 'again', 'doing', 'but', 'out', 'until', 'weren', 'its', 'themselves', \"mustn't\", 'himself', 'and', 'such', 'are', 'can', 'theirs', 'only', 'now', 'me', 'above', 'won', 'yourselves', 'most', \"that'll\", 'there'}\n"
     ]
    }
   ],
   "source": [
    "print(english_stops) # NLTK english stopwords 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불용어 제거 방법을 이해했다면 자신만의 불용어 사전을 만들어 불용어를 쉽게 제거할 수 있다. 불용어 사전은 아래와 같이 리스트로 쉽게 구현할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sorry', 'go', 'movie', 'yesterday']\n"
     ]
    }
   ],
   "source": [
    "my_stopword = ['i', 'go', 'to'] # 나만의 stopwords를 리스트로 정의해 만들고 사용 (한글 처리에도 유용함)\n",
    "\n",
    "result = [word for word in tokens if word not in english_stops]\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ed2b792a06f852f8a67625a13650f047b0b4c8cfe7bcb6127fab4c31b8414b3a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
