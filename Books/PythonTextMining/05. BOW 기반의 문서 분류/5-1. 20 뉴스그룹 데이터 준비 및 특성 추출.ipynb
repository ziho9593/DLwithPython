{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-1. 20 뉴스그룹 데이터 준비 및 특성 추출\n",
    "[The 20 newsgroups text dataset](http://scikit-learn.org/0.19/datasets/twenty_newsgroups.html)은 텍스트 마이닝에서 문서 분류의 성능을 측정하기 위해 가장 많이 사용되는 데이터셋 중 하나다. 새로운 텍스트 마이닝 기법이 개발됐을 때, 그 기법의 성능을 평가하는 기준으로 많이 쓰인다. 이 데이터셋을 이해하려면 먼저 [유즈넷](https://ko.wikipedia.org/wiki/유즈넷)을 알아야 한다. 유즈넷은 초창기 인터넷에서 이메일과 함께 가장 많이 사용된 서비스 중 하나로 일종의 게시판이라고 이해할 수 있다. 유즈넷에는 여러 뉴스 서버가 있고, 사용자는 이 중 한 서버에 접속해 하나 이상의 뉴스그룹을 구독할 수 있다. 뉴스그룹마다 특정 주제가 있으며, 사용자는 그에 맞는 뉴스를 올리거나 읽을 수 있다. 뉴스그룹이란 용어는 언론에서 출판되는 기사보다는 게시판에 올라온 사용자들의 포스트를 생각하는 것이 더 가깝다고 할 수 있다.\n",
    "\n",
    "### 데이터셋 확인 및 분리\n",
    "20 뉴스그룹은 약 20,000여 개의 뉴스그룹 문서로 이뤄져 있다. 사이킷런에서는 sklearn.datasets.fetch_20newsgroups 모듈을 통해 20개 토픽 혹은 분류에 속한 18,000여 개의 문서를 다운받을 수 있도록 지원한다. 일반적으로 머신러닝을 수행할 때에는 train dataset과 test dataset으로 데이터셋을 분리한다. 학습 데이터셋은 학습모형을 만들 때 사용하고, 평가 데이터 셋은 학습모형을 평가하는 데 쓴다. 보통 하나의 데이터셋을 랜덤하게 분리해서 두 데이터셋을 구성하는 경우가 많은데, 사이킷런에서는 학습 데이터셋과 평가 데이터셋이 미리 분리돼 있어서 서로 다른 기법의 성능을 공정하게 비교할 수 있다. 이 외 사이킷런 20 뉴스그룹 데이터는 아래와 같은 특징을 갖고 있다.\n",
    "- categories 매개변수를 이용해 20개의 topic 중에서 원하는 토픽을 선택할 수 있다.\n",
    "- remove로 필요 없는 데이터를 삭제할 수 있다.\n",
    "- 각 데이터셋 내에서 .data는 텍스트의 내용을, .target은 숫자로 표시된 라벨(분류)을 가져오는 데 사용된다.\n",
    "\n",
    "이번 실습에서는 문제를 단순하게 함으로써 속도를 높이기 위해 토픽(카테고리)을 20개 중 'alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space' 4개만 선택해 수행하고자 하며, headers나 footers에 종종 토픽의 이름이 쓰여 있는 경우가 있어 힌트를 배제하고자 remove로 제거한다. subset 매개변수는 학습 데이터셋과 평가 데이터셋을 구분하는 데 쓴다. 데이터를 가져온 다음에는 각 데이터셋의 크기, 라벨의 이름, 실젯값과 같이 가장 기본적으로 살펴봐야 할 정보를 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train set size: 2034\n",
      "# Test set size: 1353\n",
      "# Selected categories: ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
      "# Train labels: {0, 1, 2, 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "#20개의 토픽 중 선택하고자 하는 토픽을 리스트로 생성\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "#학습 데이터셋을 가져옴\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "#메일 내용에서 hint가 되는 부분을 삭제 - 순수하게 내용만으로 분류\n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)\n",
    "#검증 데이터셋을 가져옴\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', \n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                     categories=categories)\n",
    "\n",
    "print('# Train set size:', len(newsgroups_train.data))\n",
    "print('# Test set size:', len(newsgroups_test.data))\n",
    "print('# Selected categories:', newsgroups_train.target_names)\n",
    "print('# Train labels:', set(newsgroups_train.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 말했던 바와 같이 학습 데이터와 평가 데이터는 각 .data와 .target으로 문서의 본문과 라벨(토픽)의 내용을 갖는다. 카테고리와 라벨을 들여다보면, 카테고리는 문자열로 된 원래의 값이지만 라벨은 이를 숫자로 변경한 것을 볼 수 있다. 순서가 동일하므로 나중에 숫자로 된 라벨을 예측하더라도 그것이 원래 의미하는 카테고리가 무엇인지 알 수 있다.\n",
    "\n",
    "데이터 분석을 위해서는 데이터를 직접 들여다보는 것이 매우 중요하므로 아래와 같이 첫째 값을 확인해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train set text samples: Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "# Train set label smaples: 1\n",
      "# Test set text samples: TRry the SKywatch project in  Arizona.\n",
      "# Test set label smaples: 2\n"
     ]
    }
   ],
   "source": [
    "print('# Train set text samples:', newsgroups_train.data[0])\n",
    "print('# Train set label smaples:', newsgroups_train.target[0])\n",
    "print('# Test set text samples:', newsgroups_test.data[0])\n",
    "print('# Test set label smaples:', newsgroups_test.target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "newsgroups_train.target_names에는 토픽(라벨)의 값이 들어 있는데, 이 순서에 따라 .target의 숫자가 정해진다. 위에서 target_names가 ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']이고 첫 학습 데이터의 라벨이 1이므로, 이 문서는 'comp.graphics'가 해당 토픽임을 알 수 있다. 반면 평가 데이터셋의 첫 데이터는 라벨이 2이므로, 해당 토픽이 'sci.space'임을 알 수 있다.\n",
    "\n",
    "### 카운트 기반 특성 추출\n",
    "대략 내용이 파악됐다면 아래와 같이 newsgroups_train과 newsgroups_test로부터 .data와 .target을 이용해 X_train, X_test, y_train, y_test를 추축하고 앞에서 배운 특성 추출을 한 후에 실제로 문서 분류를 수행해보기로 한다. 앞에서 배웠던 내용을 골고루 사용해보기 위해 먼저 카운트 벡터를 이용해 특성을 추출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = newsgroups_train.data   #학습 데이터셋 문서\n",
    "y_train = newsgroups_train.target #학습 데이터셋 라벨\n",
    "\n",
    "X_test = newsgroups_test.data     #검증 데이터셋 문서\n",
    "y_test = newsgroups_test.target   #검증 데이터셋 라벨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set dimension: (2034, 2000)\n",
      "Test set dimension: (1353, 2000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features=2000, min_df=5, max_df=0.5)\n",
    "\n",
    "X_train_cv = cv.fit_transform(X_train) # train set을 변환\n",
    "print('Train set dimension:', X_train_cv.shape) \n",
    "X_test_cv = cv.transform(X_test) # test set을 변환\n",
    "print('Test set dimension:', X_test_cv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 예시에서는 CountVectorizer 객체를 생성함과 동시에 fit() 함수를 호출해 학습을 수행했으며, max_features를 이용해 특성의 수를 2,000개로 제한했다. min_df는 minimum document frequency로 단어가 최소 이 개수만큼의 문서에 나타나야 한다는 것을 의미한다. 어떤 단어가 문서에서 거의 쓰이지 않는다는 것은 매우 예외적인 단어일 가능성이 있다. 따라서 예시에서는 5개 미만의 문서에서 나타나는 단어는 특성에서 제외했다. min_df의 효과를 눈으로 확인하려면 max_features를 제한하지 않고 min_df 값을 조절해가며 생성된 특성의 수를 확인해보면 된다. min_df 값이 클수록 생성된 특성의 수는 적어질 것이다. max_df는 많은 문서에서 공통으로 나타나는 단어를 제외하기 위해 사용한다. 예제에서는 0.5로 문서의 50%를 초과해 나타나는 단어들은 제외했다. min_df와 max_df는 특성에 포함될 단어들을 선택하기 위해 사용되며 그 결과에 따라 문서분류의 성능도 달라지므로, 최적의 성능을 얻으려면 이러한 값들을 다양하게 적용해보는 것이 좋다. 대략 어떤 단어가 얼마나 있는지 보기 위해 4장에서 사용한 방법으로 단어의 빈도를 아래와 같이 살펴본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 : 0, 000 : 0, 01 : 0, 04 : 0, 05 : 0, 10 : 0, 100 : 0, 1000 : 0, 11 : 0, 12 : 0, 128 : 0, 129 : 0, 13 : 0, 130 : 0, 14 : 0, 15 : 0, 16 : 0, 17 : 0, 18 : 0, 19 : 0, 1987 : 0, 1988 : 0, 1989 : 0, 1990 : 0, 1991 : 0, 1992 : 0, 1993 : 0, 20 : 0, 200 : 0, 202 : 0, 21 : 0, 22 : 0, 23 : 0, 24 : 0, 25 : 0, 256 : 0, 26 : 0, 27 : 0, 28 : 0, 2d : 0, 30 : 0, 300 : 0, 31 : 0, 32 : 0, 33 : 0, 34 : 0, 35 : 0, 39 : 0, 3d : 0, 40 : 0, 400 : 0, 42 : 0, 45 : 0, 50 : 0, 500 : 0, 60 : 0, 600 : 0, 65 : 0, 70 : 0, 75 : 0, 80 : 0, 800 : 0, 90 : 0, 900 : 0, 91 : 0, 92 : 0, 93 : 0, 95 : 0, _the : 0, ability : 0, able : 1, abortion : 0, about : 1, above : 0, absolute : 0, absolutely : 0, ac : 0, accept : 0, acceptable : 0, accepted : 0, access : 0, according : 0, account : 0, accurate : 0, across : 0, act : 0, action : 0, actions : 0, active : 0, activities : 0, activity : 0, acts : 0, actual : 0, actually : 0, ad : 0, add : 0, added : 0, addition : 0, additional : 0, address : 0, "
     ]
    }
   ],
   "source": [
    "for word, count in zip(cv.get_feature_names_out()[:100], X_train_cv[0].toarray()[0, :100]):\n",
    "    print(word, ':', count, end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "관점에 따라 사용된 단어들이 별로 마음에 들지 않을 수 있다. 의미 없어 보이는 숫자가 많고, activities와 activity처럼 정규화되지 않은 결과 외에 an, as와 같이 별 의미 없는 단어들은 제외하는 것이 바람직할 수 있다. 그러나 문서의 특성, 분류의 목정 등에 따라 이러한 변수들이 성능에 미치는 영향은 달라질 수 있으므로 실제 작업에서는 상황에 따라 결정해서 수행하는 것이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'X_train' (list)\n",
      "Stored 'y_train' (ndarray)\n",
      "Stored 'X_test' (list)\n",
      "Stored 'y_test' (ndarray)\n",
      "Stored 'X_train_cv' (csr_matrix)\n",
      "Stored 'X_test_cv' (csr_matrix)\n",
      "Stored 'newsgroups_train' (Bunch)\n",
      "Stored 'newsgroups_test' (Bunch)\n"
     ]
    }
   ],
   "source": [
    "% store X_train\n",
    "% store y_train\n",
    "% store X_test\n",
    "% store y_test\n",
    "% store X_train_cv\n",
    "% store X_test_cv\n",
    "% store newsgroups_train\n",
    "% store newsgroups_test"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57bd987b9054a02bc006029d69b7bc788e59727745f4d7ba6db66b53f7f1f82b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.2 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
