{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-5. 결정트리 등을 이용한 기타 문서 분류 방법\n",
    "\n",
    "결정트리도 분류를 위한 머신러닝 알고리즘 중 하나이므로 문서 분류에 사용할 수 있다. 결정트리의 장점 중 하나는 왜 그와 같이 예측했는지에 대해 체계적인 설명이 가능하다는 것이다. 모형이 학습되면 결정트리를 그려서 분류가 되는 과정을 살펴볼 수 있다. 다만 문제는 특성이 너무 많은 경우, 이를 그려서 보기가 쉽지 않다는 점이다. 결정트리를 쓰고 싶다면 특성의 수를 비롯한 여러 가지를 고려해야 한다.\n",
    "\n",
    "사이킷런은 결정트리를 위한 [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) 클래서, 결정트리 기반의 앙상블 모형 중 하나인 랜덤포레스트를 위한 [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) 클래스, 결정트리 기반의 모형 중에서 높은 성능을 자랑하는 그레이디언트 부스틩을 지원하는 [GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) 클래스를 제공한다. 아래 코드는 이 세 가지에 대해 문서 분류를 실행한 결과를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "% store -r X_train\n",
    "% store -r X_train_tfidf\n",
    "% store -r X_test_tfidf\n",
    "% store -r y_train\n",
    "% store -r y_test\n",
    "% store -r tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Decision Tree train set score: 0.977\n",
      "# Decision Tree test set score: 0.536\n",
      "# Random Forest train set score: 0.977\n",
      "# Random Forest test set score: 0.685\n",
      "# Gradient Boosting train set score: 0.933\n",
      "# Gradient Boosting test set score: 0.696\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=7)\n",
    "tree.fit(X_train_tfidf, y_train)\n",
    "print('# Decision Tree train set score: {:.3f}'.format(tree.score(X_train_tfidf, y_train)))\n",
    "print('# Decision Tree test set score: {:.3f}'.format(tree.score(X_test_tfidf, y_test)))\n",
    "\n",
    "forest = RandomForestClassifier(random_state=7)\n",
    "forest.fit(X_train_tfidf, y_train)\n",
    "print('# Random Forest train set score: {:.3f}'.format(forest.score(X_train_tfidf, y_train)))\n",
    "print('# Random Forest test set score: {:.3f}'.format(forest.score(X_test_tfidf, y_test)))\n",
    "\n",
    "gb = GradientBoostingClassifier(random_state=7)\n",
    "gb.fit(X_train_tfidf, y_train)\n",
    "print('# Gradient Boosting train set score: {:.3f}'.format(gb.score(X_train_tfidf, y_train)))\n",
    "print('# Gradient Boosting test set score: {:.3f}'.format(gb.score(X_test_tfidf, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "space: 0.126, graphics: 0.080, atheism: 0.024, thanks: 0.023, file: 0.021, orbit: 0.020, jesus: 0.018, god: 0.018, hi: 0.017, nasa: 0.015, image: 0.015, files: 0.014, christ: 0.010, moon: 0.010, bobby: 0.010, launch: 0.010, looking: 0.010, christian: 0.010, atheists: 0.009, christians: 0.009, fbi: 0.009, 3d: 0.008, you: 0.008, not: 0.008, islamic: 0.007, religion: 0.007, spacecraft: 0.007, flight: 0.007, computer: 0.007, islam: 0.007, ftp: 0.006, color: 0.006, software: 0.005, atheist: 0.005, card: 0.005, people: 0.005, koresh: 0.005, his: 0.005, kent: 0.004, sphere: 0.004, "
     ]
    }
   ],
   "source": [
    "sorted_feature_importances = sorted(\n",
    "    zip(tfidf.get_feature_names_out(), gb.feature_importances_),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "for feature, value in sorted_feature_importances[:40]:\n",
    "    print(f'{feature}: {value:.3f}', end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 결과를 보면 상위에 있는 sapce, graphics, atheism, orbit, jesus, got, nasa 등 로지스틱 회귀분석의 결과와 일치하는 결과들이 상당히 많은 것을 볼 수 있다. 다만 결정트리의 feature_importances_는 coef_와 결정적으로 다른 점이 하나 있는데, 결정트리에서는 성격이 비슷한 두 단어 중 하나가 분류에 먼저 사용되면 다른 단어는 상대적으로 중요도가 떨어진다. 다시 말해 로지스틱 회귀분석에서는 두 단어가 비슷한 계수값을 가져도, 결정트리에서는 완전히 다른 값을 가질 수 있다.\n",
    "\n",
    "결정트리의 좋은 점은 왜 그런 결과가 나왔는지를 명확하게 제시할 수 있다는 점이다. 그러나 특성의 수가 상대적으로 매우 많은 텍스트 분류에서 완전한 결정트리가 너무 크고 단계를 전부 설명하는 것도 어려우므로 완전한 설명을 원한다면 특성의 수를 훨씬 적은 수로 제한하는 것이 좋다. 다만 그렇게 하면 정확도가 급격히 떨어진다는 단점이 있다."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57bd987b9054a02bc006029d69b7bc788e59727745f4d7ba6db66b53f7f1f82b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.2 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
