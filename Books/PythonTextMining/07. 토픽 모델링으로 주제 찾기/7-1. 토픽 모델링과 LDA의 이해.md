## 7-1. 토픽 모델링과 LDA의 이해
토픽 모델링은 텍스트 마이닝 기법 중에서 가장 많이 활용되는 기법 중 하나로, 다양한 문서 집합에 내재한 토픽, 즉 주제를 파악할 때 쓰는 방법이다. 문서 분류가 텍스트의 내용을 파악해서 무엇인가를 예측하는 것에 목적을 두었다면, 토픽 모델링은 예측보다는 내용의 분석 자체를 목적으로 하는 기법이라고 할 수 있다. 즉 주어진 텍스트에 대해 사후적으로 분석하는 기법으로 새로운 문서에 대한 예측에는 잘 쓰이지 않는다. 7장에서는 토픽 모델링 알고리즘 중에서 가장 널리 쓰이는 LDA(Latent Dirichlet Allocation) 기법을 중심으로 토픽 모델링에 대해 이해하고 실습하고자 한다.

### 토픽 모델링이란?
앞서 이 책에서 텍스트의 분석을 위해 다룬 방법은 많이 사용된 단어들의 빈도를 이용해 그래프를 그리거나 워드 클라우드를 통해 내용을 시각화하는 것이었다. 그러나 단어의 빈도만으로는 글을 쓴 의도와 전체 내용을 파악하는 데 한계가 있다. 예를 들어 '벌'과 같은 동음이의어가 나왔을 때는, 전체적인 맥락을 알아야만 정확한 이해가 가능하다. '벌'이 '꿀', '벌집' 등과 같은 단어와 쓰였다면 날아다니는 곤충의 의미임을 알 수 있고, '잘못', '책임' 등과 같이 쓰였다면 잘못한 사람에게 주는 고통의 의미임을 알 수 있다. 이와 같이 함께 사용되는 단어의 집합으로 문서에 담긴 주제를 표현하면 더 구체적이고 명확하게 의미를 보여줄 수 있다는 점이 토픽 모델링을 사용하는 중요한 이유라고 할 수 있다.

토픽 모델링은 '내제된 주제의 분석'을 가능하게 하는 기법이다. 더불어 이러한 주제들이 시간에 따라 어떻게 변화했는지를 살펴보는 '토픽 트랜드'를 분석할 수도 있다.

### LDA 모형의 구조
'**LDA(Latent Dirichlet Allocation)**'는 토픽 모델링에 가장 널리 쓰이는 기본적인 알고리즘이다. LDA의 기본 가정은, 문서들이 쓰여질 때 그 문서를 구성하는 몇 개의 토픽이 존재하며 각 토픽은 단어의 집합으로 구성됐다는 것이다. 즉 내가 문서를 작성하는 작가라고 할 때, 나는 이 문서에 넣고 싶은 주제들을 먼저 구상하고 문서를 쓴다. 또한 각 주제는 내가 하고 싶은 말을 표현하는 단어들의 집합으로 표현할 수 있다.

예를 들어, 문화도시를 구성하기 위한 문화정책에 대한 글을 쓰고 싶다면 이 글 안에 문화정책의 개념과 관련한 주제, 문화도시를 만들기 위한 전략과 관련한 주제를 포함하려고 할 것이다. 문화정책에는 일반적인 내용도 있지만 문화재와 관련한 정책, 그리고 영화산업 정책이 있을 수 있다. 이와 같은 여러 주제에 이름을 붙인다면 '문화정책 개념', '문화도시 전략', '문화재 정책', '영화산업 정책'이라고 할 수 있다. 이 중에서 '문화재 정책'이라는 주제를 쓰기 위해 '문화재', '문화유산', '유적지', '무형문화재' 등의 단어를 중심으로 글을 쓸 수 있다.

그러나 이상과 같은 내용은 가정일 뿐, 각 문서에서 토픽이 무엇이고 각 토픽은 어떤 단어들로 이루어졌다는 사실이 명시적으로 드러나지는 않는다. 그래서 '내재된 주제 혹은 토픽'이라고 부르며, LDA는 이와 같이 내재한 토픽들을 유추하고자 하는 통계적 방법론이라고 설명할 수 있다.

LDA 모형의 구조를 직관적으로 간단하게 설명하자면, 먼저 토픽 모델링은 전체 문서에 공통적으로 내재한 토픽들을 식별한다. 즉 각 문서가 개별적으로 전혀 다른 토픽들로 구성되는 것이 아니고 전체 말뭉치를 관통하는 토픽들이 있으며, 문서는 이러한 공통적인 토픽들이 다양한 비중으로 결합된 것으로, 문서에 따른 토픽의 확률분포를 추정하는 것이 토픽 모델링의 첫째 목적이 된다. 하나의 문서 안에서 그 문서의 토픽분포는 [디리클레 분포](https://ko.wikipedia.org/wiki/%EB%94%94%EB%A6%AC%ED%81%B4%EB%A0%88_%EB%B6%84%ED%8F%AC)를 따른다. 여기서 디리클레 분포는 연속 확률분포의 하나이며, k 차원의 실수 벡터에서 각 벡터의 값이 양수이고 모든 값을 더하면 1이 되는 경우에 대해 확률값이 정의되는 분포다. 문서에는 여러 개의 토픽이 있고, 각 토픽은 확률값을 갖는다. 다시 말해서 내가 세 개의 토픽을 문서에 넣을 때, 세 토픽에 대한 비중을 [0.5, 0.3, 0.2] 같은 식으로 정할 수 있다. 이때 각 토픽에 대한 확률은 모두 양수이고 값을 모두 합하면 1이 된다. 디리클레 분포는 이러한 상황을 표현하는 대표적인 연속 확률분포로 이해하면 된다. 즉 우리는 문서의 토픽 분포가 디리클레 분포를 따른다고 가정하는 것이다.

토픽 모델링의 둘째 목표는 각 토픽의 단어분포를 알아내는 것이다. 토픽은 그 토픽을 구성하는 단어들의 비중으로 표현된다. 각 토픽의 단어 분포는 문서의 토픽 분포와 같이 디리클레 분포를 따른다. 토픽에는 여러 단어가 있고 각 단어가 확률값을 가지면 모두 합하면 1이 되기 때문이다. 문서의 토픽분포와 각 토픽의 단어분포가 결합됨으로써 문서의 단어분포가 결정된다는 것이 LDA의 확률적 가정이다. 즉, 어떤 문서에 사용된 단어의 분포(각 단어의 빈도)는 이 문서에 잠재된 토픽에 의해 결정되는데, 각 토픽은 단어의 분포이므로 이것들이 결합되어 문서에 사용된 단어의 분포가 된다는 것이다.

문서에 사용된 단어들의 빈도는 텍스트의 전처리를 통해 측정이 가능하다. LDA를 이용한 토피 모델링의 내용을 요약하자면, 각 문서에 사용된 단어들의 빈도를 측정하고, 이로부터 역으로 모든 문서의 토픽 분포와 각 토픽의 단어분포를 추정하는 것이라고 할 수 있다.

토픽의 개수 K와 토픽 분포와 단어 분포의 디레클레 분포 매개변수 α, β는 하이퍼 파라미터로, 사용자가 가정하여 값을 주고 LDA를 실행하게 된다. 따라서 적절한 토픽 수, α, β를 설정함으로써 토픽 모델링의 성능을 높일 수 있다. 이때 무엇이 적절한 값인지 판단하려면 성능에 대한 척도가 필요한데, 주로 사용하는 것으로 혼란도(perplexity)와 토픽 응집도(topic coherence)가 있다.

### 모형의 평가와 적절한 토픽 수 결정
**Perplexity**는 혼란도 혹은 혼잡도로 번역되며, 보통은 특정한 확률 모형이 실제로 관측되는 값을 얼마나 유사하게 예측해내는지를 평가할 때 사용된다. 토픽 모델링에서는 우리가 추정한 디리클레 모형이 주어진 문서 집합을 얼마나 유사하게 생성할 수 있는지 나타낸다고 해석할 수 있다. 다만 이름이 혼란도이므로 값이 작을수록 토픽 모델이 문서집합을 잘 반영한다고 생각하면 된다.

토픽 응집도(coherence)는 각 토픽에서 상위 비중을 차지하는 단어들이 의미적으로 유사한지를 나타내는 척도다. 만일 토픽이 단일 주제를 잘 표현한다면 의미적으로 유사한 단어들의 비중이 높을 것이라는 가정에 따라 성능을 표현한다. 이 값은 혼란도와는 달리 값이 클수록 좋다.

토픽 모형의 성능을 평가할 때 무엇보다 중요한 점이 있는데, 혼란도나 토픽 응집도와 같은 척도의 수치보다, 토픽의 해석이 사람이 보기에 자연스러운 것이 더 중요하다는 것이다. 따라서 실제로 토픽 모델링을 할 때에는 척도에 따라 모형을 바로 선택하기보다 최적값 근처에서 몇 개의 값을 선택해 모델링을 수행하고 그 결과를 사람이 직접 비교해보는 것이 좋다. 이는 토픽 모델링의 성능과 관련한 논문들에서도 중요하게 지적되는 사항이다.

이 책에서는 사이킷런과 Gensim의 두 패키지로 토픽 모델링을 실습해본다. 사이킷런은 혼란도 계산만 제공하고 토픽 응집도는 지원하지 않는다. 반면, Gensim은 두 척도를 모두 지원하므로 다양하게 해보고 본인에게 맞는 것을 선택하는 것이 좋다. 일반적으로는 Gensim이 더 많이 사용되나, 사이킷런을 사용하면 지금까지 배운 내용과 자연스럽게 연결된다는 장점이 있다.